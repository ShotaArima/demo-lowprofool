{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LowProFool\n",
    "## Adversarial examples generation on the german credit dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# Keras \n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "from Adverse import lowProFool, deepfool\n",
    "from Metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAADLCAYAAACyEVKsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAB7CAAAewgFu0HU+AAAEFklEQVR4nO3aMW5jVRiA0TsohWlGNNbIo1hKmcYSTbwFWAw9O2AFLIaaIoUrJO+ByHJHMy4sebqEQEHh8PwpPqd6r7n3L6xPV/f5w+l0Og0AUr659AAA/Js4AwSJM0CQOAMEiTNAkDgDBIkzQJA4AwSJM0CQOAMEiTNAkDgDBIkzQJA4AwSJM0CQOAME3Uy94eFwGNvtdowxxnw+Hzc3k48A8KaOx+PY7/djjDFWq9WYzWZnrzl5Gbfb7Viv11NvCzCJzWYzHh4ezl7HtQZA0OQn5/l8/vLy029jfPw09QhcmT9++fnSI/DO7cZh/DB+H2P8o3FnmDzOr+6YP34a47vPU4/Alfk8vr30CFyRt/qO5loDIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkg6GbqDY/H48vLX7upt+cK/Tm+XHoE3rndODw/v2rcGSaP836/f3n59cept+cKfX/pAbgq+/1+3N3dnb3O5Ncau53TMsB/mfzkfH9///z8+Pg4lsvl1CNwJZ6ensZ6vR5jjLHZbMZisbjwRLxXx+Px+VZgtVq9yZqTx3k2mz0/L5fLcXt7O/UIXKHFYuG3xv/qLa4y/s6/NQCCxBkgSJwBgsQZIEicAYLEGSBInAGCPpxOp9OlhwDgNSdngCBxBggSZ4AgcQYIEmeAIHEGCBJngCBxBggSZ4AgcQYIEmeAIHEGCBJngCBxBggSZ4AgcQYIEmeAIHEGCBJngKCvqKRJBjR40jQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 200x100 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 101,
       "width": 179
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retina display\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "pd.set_option('display.max_columns', 500)\n",
    "tqdm.pandas()\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "ccolors = [\"#008ae9\", \"#ea004f\"]\n",
    "sns.set_palette(ccolors)\n",
    "sns.palplot(sns.color_palette())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "DATASET = 'credit-g'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data from openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(dataset):\n",
    "    assert dataset == 'credit-g', \"This function is specifically for the credit-g dataset\"\n",
    "    \n",
    "    dataset = fetch_openml(dataset, as_frame=True)\n",
    "    df = dataset.data\n",
    "    df['target'] = dataset.target\n",
    "\n",
    "    # Renaming target for training later\n",
    "    df['target'] = df['target'].map({'bad': 0.0, 'good': 1.0})\n",
    "\n",
    "    # Subsetting features to keep only continuous, discrete and ordered categorical\n",
    "    feature_names = ['checking_status', 'duration', 'credit_amount',\n",
    "                     'savings_status', 'employment', 'installment_commitment',\n",
    "                     'residence_since', 'age', 'existing_credits', 'num_dependents',\n",
    "                     'own_telephone', 'foreign_worker']\n",
    "\n",
    "    df = df[feature_names + ['target']]\n",
    "\n",
    "    # Convert categorical variables to numeric\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = pd.Categorical(df[col]).codes\n",
    "\n",
    "    return df, 'target', feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, target, feature_names, bounds):\n",
    "    df_return = df.copy()\n",
    "    \n",
    "    # Convert '<0' to NaN or appropriate values\n",
    "    for col in feature_names:\n",
    "        df_return[col] = pd.to_numeric(df_return[col], errors='coerce')\n",
    "    \n",
    "    # Makes sure target does not need scaling\n",
    "    targets = np.unique(df_return[target].values)\n",
    "    assert len(targets) == 2 and 0. in targets and 1. in targets\n",
    "\n",
    "    # Check if feature_names matches the columns in df_return\n",
    "    assert set(feature_names) <= set(df_return.columns), \"Some feature_names are not columns in df_return\"\n",
    "    \n",
    "    # Select only the columns in feature_names\n",
    "    X = df_return[feature_names]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    df_return[feature_names] = X_scaled\n",
    "    \n",
    "    lower_bounds = scaler.transform([bounds[0]])\n",
    "    upper_bounds = scaler.transform([bounds[1]])\n",
    "\n",
    "    return scaler, df_return, (lower_bounds[0], upper_bounds[0])\n",
    "\n",
    "\n",
    "def get_weights(df, target, show_heatmap=False):\n",
    "    def heatmap(cor):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "        plt.show()\n",
    "\n",
    "    cor = df.corr()\n",
    "    cor_target = abs(cor[target])\n",
    "\n",
    "    weights = cor_target[cor_target.index != target]\n",
    "    weights = weights / np.linalg.norm(weights)\n",
    "\n",
    "    if show_heatmap:\n",
    "        heatmap(cor)\n",
    "            \n",
    "    return weights.values\n",
    "\n",
    "def balance_df(df, target):\n",
    "    len_df_0, len_df_1 = len(df[df[target] == 0]), len(df[df[target] == 1])\n",
    "    min_samples = min(len_df_0, len_df_1)\n",
    "    df_0 = df[df[target] == 0].sample(min_samples, random_state=SEED)\n",
    "    df_1 = df[df[target] == 1].sample(min_samples, random_state=SEED)\n",
    "    return pd.concat((df_0, df_1))\n",
    "\n",
    "def get_bounds(df, feature_names):\n",
    "    return [df[feature_names].min().values, df[feature_names].max().values]\n",
    "# def get_bounds():\n",
    "#     low_bounds = df_orig.min().values\n",
    "#     up_bounds = df_orig.max().values\n",
    "    \n",
    "#     #removing target WARNING ASSUMES TARGET IS LAST\n",
    "#     low_bounds = low_bounds[:-1]\n",
    "#     up_bounds = up_bounds[:-1]\n",
    "    \n",
    "#     return [low_bounds, up_bounds]\n",
    "\n",
    "def split_train_test_valid(df, test_size=300, valid_size=50):\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, shuffle=True, random_state=SEED)\n",
    "    df_test, df_valid = train_test_split(df_test, test_size=valid_size, shuffle=True, random_state=SEED)\n",
    "    return df_train, df_test, df_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(conf, load=False):\n",
    "    assert(conf['Dataset'] == 'credit-g')\n",
    "    \n",
    "    class GermanNet(nn.Module):\n",
    "        def __init__(self, D_in, H, D_out):\n",
    "            super(GermanNet, self).__init__()\n",
    "            self.linear1 = torch.nn.Linear(D_in, H)\n",
    "            self.linear2 = torch.nn.Linear(H, H)\n",
    "            self.linear3 = torch.nn.Linear(H, D_out)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.softmax = torch.nn.Softmax(dim=0)\n",
    "\n",
    "        def forward(self, x):\n",
    "            h1 = self.relu(self.linear1(x))\n",
    "            h2 = self.relu(self.linear2(h1))\n",
    "            h3 = self.relu(self.linear2(h2))\n",
    "            h4 = self.relu(self.linear2(h3))\n",
    "            h5 = self.relu(self.linear2(h4))\n",
    "            h6 = self.relu(self.linear2(h5))\n",
    "            a3 = self.linear3(h6)\n",
    "            y = self.softmax(a3)\n",
    "            return y\n",
    "\n",
    "    def train(model, criterion, optimizer, X, y, N, n_classes):\n",
    "        model.train()\n",
    "\n",
    "        current_loss = 0\n",
    "        current_correct = 0\n",
    "\n",
    "\n",
    "        # Training in batches\n",
    "        for ind in range(0, X.size(0), N):\n",
    "            indices = range(ind, min(ind + N, X.size(0)) - 1) \n",
    "            inputs, labels = X[indices], y[indices]\n",
    "            inputs = Variable(inputs, requires_grad=True)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(inputs)\n",
    "            _, indices = torch.max(output, 1) # argmax of output [[0.61, 0.12]] -> [0]\n",
    "            # [[0, 1, 1, 0, 1, 0, 0]] -> [[1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0]]\n",
    "            preds = torch.tensor(keras.utils.to_categorical(indices, num_classes=n_classes))\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss += loss.item()\n",
    "            current_correct += (preds.int() == labels.int()).sum() /n_classes\n",
    "\n",
    "\n",
    "        current_loss = current_loss / X.size(0)\n",
    "        current_correct = current_correct.double() / X.size(0)    \n",
    "\n",
    "        return preds, current_loss, current_correct.item()\n",
    "    \n",
    "    df = conf['TrainData']\n",
    "    target = conf['Target']\n",
    "    feature_names = conf['FeatureNames']\n",
    "                        \n",
    "    n_classes = len(np.unique(df[target]))\n",
    "    X_train = torch.FloatTensor(df[feature_names].values)\n",
    "    y_train = keras.utils.to_categorical(df[target], n_classes)\n",
    "    y_train = torch.FloatTensor(y_train)\n",
    "\n",
    "    D_in = X_train.size(1)\n",
    "    D_out = y_train.size(1)\n",
    "\n",
    "    epochs = 400\n",
    "    batch_size = 100\n",
    "    H = 100\n",
    "    net = GermanNet(D_in, H, D_out)\n",
    "\n",
    "    lr = 1e-4    \n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        preds, epoch_loss, epoch_acc = train(net, criterion, optimizer, X_train, y_train, batch_size, n_classes)     \n",
    "        if (epoch % 50 == 0):\n",
    "            print(\"> epoch {:.0f}\\tLoss {:.5f}\\tAcc {:.5f}\".format(epoch, epoch_loss, epoch_acc))\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv(config, method):\n",
    "    df_test = config['TestData']\n",
    "    extra_cols = ['orig_pred', 'adv_pred', 'iters']    \n",
    "    model = config['Model']\n",
    "    weights = config['Weights']\n",
    "    bounds = config['Bounds']\n",
    "    maxiters = config['MaxIters']\n",
    "    alpha = config['Alpha']\n",
    "    lambda_ = config['Lambda']\n",
    "    \n",
    "    results = np.zeros((len(df_test), len(feature_names) + len(extra_cols)))    \n",
    "            \n",
    "    i = -1\n",
    "    for _, row in tqdm_notebook(df_test.iterrows(), total=df_test.shape[0], desc=\"{}\".format(method)):\n",
    "        i += 1\n",
    "        x_tensor = torch.FloatTensor(row[config['FeatureNames']])   \n",
    "        \n",
    "        if method == 'LowProFool':\n",
    "            orig_pred, adv_pred, x_adv, loop_i = lowProFool(x_tensor, model, weights, bounds,\n",
    "                                                             maxiters, alpha, lambda_)\n",
    "        elif method == 'Deepfool':\n",
    "            orig_pred, adv_pred, x_adv, loop_i = deepfool(x_tensor, model, maxiters, alpha,\n",
    "                                                          bounds, weights=[])\n",
    "        else:\n",
    "            raise Exception(\"Invalid method\", method)\n",
    "        results[i] = np.concatenate((x_adv, [orig_pred, adv_pred, loop_i]), axis=0)\n",
    "        \n",
    "    return pd.DataFrame(results, index=df_test.index, columns = feature_names + extra_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current definition of get_bounds:\n",
      "('df', 'feature_names', 'bounds', 'feature')\n",
      "Number of arguments: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/sklearn/datasets/_openml.py:421: UserWarning: Multiple active versions of the dataset matching the name credit-g exist. Versions may be fundamentally different, returning version 1.\n",
      "  \" {version}.\".format(name=name, version=res[0][\"version\"])\n",
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/ipykernel_launcher.py:55: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n"
     ]
    }
   ],
   "source": [
    "# Load initial dataset\n",
    "df_orig, target, feature_names = get_df(DATASET)\n",
    "\n",
    "# Balance dataset classes\n",
    "df = balance_df(df_orig, target)\n",
    "\n",
    "# 1. 正しい関数定義\n",
    "def get_bounds(df, feature_names):\n",
    "    \"\"\"\n",
    "    Calculate the bounds for given features in a dataframe, handling both numeric and categorical variables.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        feature_names (list): List of feature names to calculate bounds for\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing bounds for numeric features and unique values for categorical features\n",
    "    \"\"\"\n",
    "    bounds = {}\n",
    "    \n",
    "    for feature in feature_names:\n",
    "        if feature not in df.columns:\n",
    "            print(f\"Warning: Feature '{feature}' not found in the dataframe.\")\n",
    "            continue\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "            bounds[feature] = {\n",
    "                'type': 'numeric',\n",
    "                'min': df[feature].min(),\n",
    "                'max': df[feature].max()\n",
    "            }\n",
    "        else:\n",
    "            bounds[feature] = {\n",
    "                'type': 'categorical',\n",
    "                'values': df[feature].unique().tolist()\n",
    "            }\n",
    "    \n",
    "    return bounds\n",
    "\n",
    "# 2. 関数の呼び出し\n",
    "bounds = get_bounds(df_orig, feature_names)\n",
    "\n",
    "# 3. デバッグ用の関数\n",
    "def debug_get_bounds():\n",
    "    print(\"Current definition of get_bounds:\")\n",
    "    print(get_bounds.__code__.co_varnames)\n",
    "    print(\"Number of arguments:\", get_bounds.__code__.co_argcount)\n",
    "\n",
    "# デバッグ関数の呼び出し\n",
    "debug_get_bounds()\n",
    "\n",
    "# 4. エラーハンドリングを追加した版\n",
    "def get_bounds_safe(df, feature_names):\n",
    "    try:\n",
    "        return [df[feature_names].min().values, df[feature_names].max().values]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_bounds: {e}\")\n",
    "        print(f\"DataFrame columns: {df.columns}\")\n",
    "        print(f\"Requested features: {feature_names}\")\n",
    "        return None\n",
    "\n",
    "# 安全版の関数の呼び出し\n",
    "bounds = get_bounds_safe(df_orig, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/sklearn/preprocessing/_data.py:461: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/sklearn/preprocessing/_data.py:462: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1336/3435303739.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Normalize the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Compute the weihts modelizing the expert's knowledge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1336/2458372523.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(df, target, feature_names, bounds)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdf_return\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlower_bounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mupper_bounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Load initial dataset\n",
    "df_orig, target, feature_names = get_df(DATASET)\n",
    "\n",
    "# Balance dataset classes\n",
    "df = balance_df(df_orig, target)\n",
    "\n",
    "# Compute the bounds for clipping\n",
    "bounds = get_bounds(df_orig, feature_names)\n",
    "\n",
    "# Normalize the data\n",
    "scaler, df, bounds = normalize(df, target, feature_names, bounds)\n",
    "\n",
    "# Compute the weihts modelizing the expert's knowledge\n",
    "weights = get_weights(df, target)\n",
    "\n",
    "# Split df into train/test/valid\n",
    "df_train, df_test, df_valid = split_train_test_valid()\n",
    "\n",
    "# Build experimenation config\n",
    "config = {'Dataset'     : 'credit-g',\n",
    "         'MaxIters'     : 20000,\n",
    "         'Alpha'        : 0.001,\n",
    "         'Lambda'       : 8.5,\n",
    "         'TrainData'    : df_train,\n",
    "         'TestData'     : df_test,\n",
    "         'ValidData'    : df_valid,\n",
    "         'Scaler'       : scaler,\n",
    "         'FeatureNames' : feature_names,\n",
    "         'Target'       : target,\n",
    "         'Weights'      : weights,\n",
    "         'Bounds'       : bounds}\n",
    "\n",
    "# Train neural network\n",
    "model = get_model(config)\n",
    "config['Model'] = model\n",
    "\n",
    "# Compute accuracy on test set\n",
    "y_true = df_test[target]\n",
    "x_test = torch.FloatTensor(df_test[feature_names].values)\n",
    "y_pred = model(x_test)\n",
    "y_pred = np.argmax(y_pred.detach().numpy(), axis=1)\n",
    "print(\"Accuracy score on test data\", accuracy_score(y_true, y_pred))\n",
    "    \n",
    "# Sub sample\n",
    "config['TestData'] = config['TestData'].sample(n=10, random_state = SEED)\n",
    "\n",
    "# Generate adversarial examples\n",
    "df_adv_lpf = gen_adv(config, 'LowProFool')\n",
    "df_adv_df = gen_adv(config, 'Deepfool')\n",
    "config['AdvData'] = {'LowProFool' : df_adv_lpf, 'Deepfool' : df_adv_df}\n",
    "\n",
    "# Compute metrics\n",
    "list_metrics = {'SuccessRate' : True,\n",
    "                'iter_means': False,\n",
    "                'iter_std': False,\n",
    "                'normdelta_median': False,\n",
    "                'normdelta_mean': True,\n",
    "                'n_std': True,\n",
    "                'weighted_median': False,\n",
    "                'weighted_mean': True,\n",
    "                'w_std': True,\n",
    "                'mean_dists_at_org': False,\n",
    "                'median_dists_at_org': False,\n",
    "                'mean_dists_at_tgt': False,\n",
    "                'mean_dists_at_org_weighted': True,\n",
    "                'mdow_std': True,\n",
    "                'median_dists_at_org_weighted': False,\n",
    "                'mean_dists_at_tgt_weighted': True,\n",
    "                'mdtw_std': True,\n",
    "                'prop_same_class_arg_org': False,\n",
    "                'prop_same_class_arg_adv': False}\n",
    "\n",
    "all_metrics = get_metrics(config, list_metrics)\n",
    "all_metrics = pd.DataFrame(all_metrics, columns=['Method'] + [k for k, v in list_metrics.items() if v])\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_341/4044212574.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplot_ratios\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mm_lpf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LowProFool'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mm_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethod\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'Deepfool'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "plot_ratios = []\n",
    "\n",
    "m_lpf = all_metrics[all_metrics.Method == 'LowProFool']\n",
    "m_df = all_metrics[all_metrics.Method =='Deepfool']\n",
    "\n",
    "sr = m_lpf.SuccessRate.values / m_df.SuccessRate.values \n",
    "wm =  m_lpf.weighted_mean.values / m_df.weighted_mean.values \n",
    "\n",
    "plot_ratios.append([100*sr[0], 100*wm[0]])\n",
    "plot_ratios = pd.DataFrame(plot_ratios, columns=['Success Rate Ratio', 'Mean Perturbation Ratio'])\n",
    "plot_ratios['Dataset'] = 'German Credit'\n",
    "\n",
    "f = plt.figure()\n",
    "ax = plt.axes()\n",
    "plot_ratios.plot(x='Dataset', kind='bar', legend=True, ax=ax)\n",
    "\n",
    "for i, v in enumerate(plot_ratios['Success Rate Ratio'].values):\n",
    "    ax.text(i - 0.2, v - 12 , str(v.round(1)) + '%', fontsize=16, color='white', weight='bold')\n",
    "for i, v in enumerate(plot_ratios['Mean Perturbation Ratio'].values):\n",
    "    ax.text(i + 0.062, v - 12, str(v.round(1)) + '%', fontsize=16, color='white', weight='bold')\n",
    "\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=0, ha='center')\n",
    "ax.axhline(100, ls=':', c='grey')\n",
    "ax.text(-0.49, 100 - 5, '100%')\n",
    "\n",
    "\n",
    "ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
