{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LowProFool\n",
    "## Adversarial examples generation on the german credit dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# Keras \n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "from Adverse import lowProFool, deepfool\n",
    "from Metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAADLCAYAAACyEVKsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAB7CAAAewgFu0HU+AAAEFklEQVR4nO3aMW5jVRiA0TsohWlGNNbIo1hKmcYSTbwFWAw9O2AFLIaaIoUrJO+ByHJHMy4sebqEQEHh8PwpPqd6r7n3L6xPV/f5w+l0Og0AUr659AAA/Js4AwSJM0CQOAMEiTNAkDgDBIkzQJA4AwSJM0CQOAMEiTNAkDgDBIkzQJA4AwSJM0CQOAME3Uy94eFwGNvtdowxxnw+Hzc3k48A8KaOx+PY7/djjDFWq9WYzWZnrzl5Gbfb7Viv11NvCzCJzWYzHh4ezl7HtQZA0OQn5/l8/vLy029jfPw09QhcmT9++fnSI/DO7cZh/DB+H2P8o3FnmDzOr+6YP34a47vPU4/Alfk8vr30CFyRt/qO5loDIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkg6GbqDY/H48vLX7upt+cK/Tm+XHoE3rndODw/v2rcGSaP836/f3n59cept+cKfX/pAbgq+/1+3N3dnb3O5Ncau53TMsB/mfzkfH9///z8+Pg4lsvl1CNwJZ6ensZ6vR5jjLHZbMZisbjwRLxXx+Px+VZgtVq9yZqTx3k2mz0/L5fLcXt7O/UIXKHFYuG3xv/qLa4y/s6/NQCCxBkgSJwBgsQZIEicAYLEGSBInAGCPpxOp9OlhwDgNSdngCBxBggSZ4AgcQYIEmeAIHEGCBJngCBxBggSZ4AgcQYIEmeAIHEGCBJngCBxBggSZ4AgcQYIEmeAIHEGCBJngKCvqKRJBjR40jQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 200x100 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 101,
       "width": 179
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retina display\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "pd.set_option('display.max_columns', 500)\n",
    "tqdm.pandas()\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "ccolors = [\"#008ae9\", \"#ea004f\"]\n",
    "sns.set_palette(ccolors)\n",
    "sns.palplot(sns.color_palette())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "DATASET = 'credit-g'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data from openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(dataset):\n",
    "    assert dataset == 'credit-g', \"This function is specifically for the credit-g dataset\"\n",
    "    \n",
    "    dataset = fetch_openml(dataset, as_frame=True)\n",
    "    df = dataset.data\n",
    "    df['target'] = dataset.target\n",
    "\n",
    "    # Renaming target for training later\n",
    "    df['target'] = df['target'].map({'bad': 0.0, 'good': 1.0})\n",
    "\n",
    "    # Subsetting features to keep only continuous, discrete and ordered categorical\n",
    "    feature_names = ['checking_status', 'duration', 'credit_amount',\n",
    "                     'savings_status', 'employment', 'installment_commitment',\n",
    "                     'residence_since', 'age', 'existing_credits', 'num_dependents',\n",
    "                     'own_telephone', 'foreign_worker']\n",
    "\n",
    "    df = df[feature_names + ['target']]\n",
    "\n",
    "    # Convert categorical variables to numeric\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            print(f\"Column {col} contains non-numeric data: {df[col].unique()}\")\n",
    "            df[col] = pd.Categorical(df[col]).codes\n",
    "\n",
    "    return df, 'target', feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(df, target, show_heatmap=False):\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"Column '{target}' not found in dataframe. Available columns: {df.columns}\")\n",
    "\n",
    "    def heatmap(cor):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "        plt.show()\n",
    "\n",
    "    cor = df.corr()\n",
    "    cor_target = abs(cor[target])\n",
    "\n",
    "    weights = cor_target[cor_target.index != target]\n",
    "    weights = weights / np.linalg.norm(weights)\n",
    "\n",
    "    if show_heatmap:\n",
    "        heatmap(cor)\n",
    "            \n",
    "    return weights.values\n",
    "\n",
    "def balance_df(df, target):\n",
    "    len_df_0, len_df_1 = len(df[df[target] == 0]), len(df[df[target] == 1])\n",
    "    min_samples = min(len_df_0, len_df_1)\n",
    "    df_0 = df[df[target] == 0].sample(min_samples, random_state=SEED)\n",
    "    df_1 = df[df[target] == 1].sample(min_samples, random_state=SEED)\n",
    "    return pd.concat((df_0, df_1))\n",
    "\n",
    "def split_train_test_valid(df, test_size=300, valid_size=50):\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, shuffle=True, random_state=SEED)\n",
    "    df_test, df_valid = train_test_split(df_test, test_size=valid_size, shuffle=True, random_state=SEED)\n",
    "    return df_train, df_test, df_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, target, feature_names, bounds):\n",
    "    df_return = df.copy()\n",
    "\n",
    "    numeric_features = df_return[feature_names].select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_return[numeric_features] = scaler.fit_transform(df_return[numeric_features])\n",
    "    \n",
    "    X = df_return[feature_names]\n",
    "    scaler.fit(X)\n",
    "    df_return[feature_names] = scaler.transform(X)\n",
    "    lower_bounds = scaler.transform([bounds[0]])\n",
    "    upper_bounds = scaler.transform([bounds[1]])\n",
    "    \n",
    "    # # boundsがリスト形式の場合の処理\n",
    "    # if isinstance(bounds, list) and len(bounds) == 2:\n",
    "    #     min_bounds, max_bounds = bounds\n",
    "    #     numeric_features = [f for f in feature_names if pd.api.types.is_numeric_dtype(df[f])]\n",
    "    #     bounds_dict = {\n",
    "    #         feature: {'type': 'numeric', 'min': min_val, 'max': max_val}\n",
    "    #         for feature, min_val, max_val in zip(numeric_features, min_bounds, max_bounds)\n",
    "    #     }\n",
    "    #     # カテゴリカル特徴量に対するboundsを追加\n",
    "    #     for feature in feature_names:\n",
    "    #         if feature not in bounds_dict:\n",
    "    #             bounds_dict[feature] = {'type': 'categorical', 'values': df[feature].unique().tolist()}\n",
    "    # else:\n",
    "    #     bounds_dict = bounds  # 既存の辞書形式の場合\n",
    "\n",
    "    # # 数値特徴量とカテゴリカル特徴量を分離\n",
    "    # numeric_features = [f for f in feature_names if pd.api.types.is_numeric_dtype(df[f])]\n",
    "    # categorical_features = [f for f in feature_names if f not in numeric_features]\n",
    "\n",
    "    # # print(\"Numeric features:\", numeric_features)\n",
    "    # print(\"Categorical features:\", categorical_features)\n",
    "\n",
    "    # # スケーリング\n",
    "    # scaler = StandardScaler()\n",
    "    # if numeric_features:\n",
    "    #     df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
    "\n",
    "    # # boundsの更新\n",
    "    # scaled_bounds = {}\n",
    "    # if numeric_features:\n",
    "    #     original_mins = pd.DataFrame({f: [bounds_dict[f]['min']] for f in numeric_features})\n",
    "    #     original_maxs = pd.DataFrame({f: [bounds_dict[f]['max']] for f in numeric_features})\n",
    "    #     scaled_mins = scaler.transform(original_mins)\n",
    "    #     scaled_maxs = scaler.transform(original_maxs)\n",
    "        \n",
    "    #     for feature, scaled_min, scaled_max in zip(numeric_features, scaled_mins[0], scaled_maxs[0]):\n",
    "    #         scaled_bounds[feature] = {'type': 'numeric', 'min': scaled_min, 'max': scaled_max}\n",
    "\n",
    "    # # カテゴリカル特徴量の処理（ここではそのまま保持）\n",
    "    # for feature in categorical_features:\n",
    "    #     scaled_bounds[feature] = bounds_dict[feature]\n",
    "\n",
    "    # return scaler, df, scaled_bounds\n",
    "\n",
    "    return scaler, df_return, (lower_bounds, upper_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(conf, load=False):\n",
    "    assert(conf['Dataset'] == 'credit-g')\n",
    "    \n",
    "    class GermanNet(nn.Module):\n",
    "        def __init__(self, D_in, H, D_out):\n",
    "            super(GermanNet, self).__init__()\n",
    "            self.linear1 = torch.nn.Linear(D_in, H)\n",
    "            self.linear2 = torch.nn.Linear(H, H)\n",
    "            self.linear3 = torch.nn.Linear(H, D_out)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.softmax = torch.nn.Softmax(dim=0) # softmax関数の次元の修正 行方向に適用するため1\n",
    "\n",
    "        def forward(self, x):\n",
    "            h1 = self.relu(self.linear1(x))\n",
    "            h2 = self.relu(self.linear2(h1))\n",
    "            h3 = self.relu(self.linear2(h2))\n",
    "            h4 = self.relu(self.linear2(h3))\n",
    "            h5 = self.relu(self.linear2(h4))\n",
    "            h6 = self.relu(self.linear2(h5))\n",
    "            a3 = self.linear3(h6)\n",
    "            y = self.softmax(a3)\n",
    "            # return self.linear3(h6)\n",
    "            return y\n",
    "\n",
    "    def train(model, criterion, optimizer, X, y, N):\n",
    "        model.train()\n",
    "\n",
    "        current_loss = 0\n",
    "        current_correct = 0\n",
    "\n",
    "\n",
    "        # Training in batches\n",
    "        for ind in range(0, X.size(0), N):\n",
    "            indices = range(ind, min(ind + N, X.size(0))) \n",
    "            inputs, labels = X[indices], y[indices]\n",
    "            inputs = Variable(inputs, requires_grad=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(inputs)\n",
    "            # _, indices = torch.max(output, 1) # argmax of output [[0.61, 0.12]] -> [0]\n",
    "            # # [[0, 1, 1, 0, 1, 0, 0]] -> [[1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0]]\n",
    "            # preds = torch.tensor(keras.utils.to_categorical(indices, num_classes=n_classes))\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss += loss.item()\n",
    "            preds = (torch.sigmoid(output) > 0.5).float()\n",
    "            current_correct += (preds == labels).float().sum()\n",
    "\n",
    "        current_loss = current_loss / (X.size(0) // N)\n",
    "        current_correct = current_correct.item() / X.size(0)    \n",
    "\n",
    "        return current_loss, current_correct\n",
    "    \n",
    "    df = conf['TrainData']\n",
    "    target = conf['Target']\n",
    "    feature_names = conf['FeatureNames']\n",
    "                        \n",
    "    # n_classes = len(np.unique(df[target]))\n",
    "    X_train = torch.FloatTensor(df[conf['FeatureNames']].values) # この行はそのままでOK\n",
    "    # y_train = keras.utils.to_categorical(df[target], n_classes)\n",
    "    y_train = torch.FloatTensor(df[target].values).unsqueeze(1)\n",
    "\n",
    "    D_in = X_train.size(1)\n",
    "    D_out = 1\n",
    "\n",
    "    epochs = 400\n",
    "    batch_size = 100\n",
    "    H = 100\n",
    "    net = GermanNet(D_in, H, D_out)\n",
    "\n",
    "    lr = 1e-4\n",
    "    # criterion = torch.nn.BCEWithLogitsLoss() # BCELossから変更\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss, epoch_acc = train(net, criterion, optimizer, X_train, y_train, batch_size)     \n",
    "        if (epoch % 50 == 0):\n",
    "            print(\"> epoch {:.0f}\\tLoss {:.5f}\\tAcc {:.5f}\".format(epoch, epoch_loss, epoch_acc))\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv(config, method):\n",
    "    df_test = config['TestData']\n",
    "    feature_names = config['FeatureNames']\n",
    "    extra_cols = ['orig_pred', 'adv_pred', 'iters']    \n",
    "    model = config['Model']\n",
    "    weights = config['Weights']\n",
    "    bounds = config['Bounds']\n",
    "\n",
    "    encoded_bounds = {}\n",
    "    for feature, info in bounds.items():\n",
    "        if info['type'] == 'numeric':\n",
    "            encoded_bounds[feature] = info\n",
    "        elif info['type'] == 'categorical':\n",
    "            for value in info['values']:\n",
    "                encoded_bounds[f\"{feature}_{value}\"] = {'type': 'numeric', 'min':0, 'max':1}\n",
    "\n",
    "    maxiters = config['MaxIters']\n",
    "    alpha = config['Alpha']\n",
    "    lambda_ = config['Lambda']\n",
    "    \n",
    "    results = np.zeros((len(df_test), len(feature_names) + len(extra_cols)))\n",
    "\n",
    "    i = -1\n",
    "    for _, row in tqdm_notebook(df_test.iterrows(), total=df_test.shape[0], desc=\"{}\".format(method)):\n",
    "        i += 1\n",
    "        x_tensor = torch.FloatTensor(row[config['FeatureNames']])\n",
    "\n",
    "        # print(\"Shape of x_tensor:\", x_tensor.shape)\n",
    "        \n",
    "        if method == 'LowProFool':\n",
    "            orig_pred, adv_pred, x_adv, loop_i = lowProFool(x_tensor, model, weights, encoded_bounds, maxiters, alpha, lambda_)\n",
    "        elif method == 'Deepfool':\n",
    "            orig_pred, adv_pred, x_adv, loop_i = deepfool(x_tensor, model, maxiters, alpha, encoded_bounds, weights=[])\n",
    "        else:\n",
    "            raise Exception(\"Invalid method\", method)\n",
    "    \n",
    "        # x_advがTensorの場合、NumPy配列に変換\n",
    "        if isinstance(x_adv, torch.Tensor):\n",
    "            x_adv = x_adv.cpu().numpy()\n",
    "\n",
    "        # x_advを1次元に平坦化\n",
    "        x_adv_flat = x_adv.flatten()\n",
    "\n",
    "        # 結果を結合\n",
    "        results[i] = np.concatenate((x_adv_flat, [orig_pred, adv_pred, loop_i]))\n",
    "\n",
    "        if i == 0: # 最初の反復でのみ結果を表示\n",
    "            print(\"First row of results:\", results[0])\n",
    "            break;\n",
    "        \n",
    "    return pd.DataFrame(results, index=df_test.index, columns = feature_names + extra_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_preprocess_data(df, feature_names):\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col in feature_names:\n",
    "        if df[col].dtype == 'object':\n",
    "            # '<0' を 0 に置き換える\n",
    "            df[col] = df[col].replace('<0', '0')\n",
    "            \n",
    "            # カテゴリカルデータを数値にエンコード\n",
    "            df[col] = pd.Categorical(df[col]).codes\n",
    "        \n",
    "        # 数値型に変換\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # 欠損値を処理（例：中央値で埋める）\n",
    "    df = df.fillna(df.median())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# データのクリーニングと前処理を適用\n",
    "df = clean_and_preprocess_data(df, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_categorical(df, feature_names):\n",
    "    le = LabelEncoder()\n",
    "    for col in feature_names:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "    return df\n",
    "\n",
    "# df = encode_categorical(df, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/sklearn/datasets/_openml.py:421: UserWarning: Multiple active versions of the dataset matching the name credit-g exist. Versions may be fundamentally different, returning version 1.\n",
      "  \" {version}.\".format(name=name, version=res[0][\"version\"])\n",
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/ipykernel_launcher.py:55: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n"
     ]
    }
   ],
   "source": [
    "# Load initial dataset\n",
    "df_orig, target, feature_names = get_df(DATASET)\n",
    "\n",
    "# Balance dataset classes\n",
    "df = balance_df(df_orig, target)\n",
    "\n",
    "# 1. 正しい関数定義\n",
    "def get_bounds(df, feature_names):\n",
    "    \"\"\"\n",
    "    Calculate the bounds for given features in a dataframe, handling both numeric and categorical variables.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        feature_names (list): List of feature names to calculate bounds for\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing bounds for numeric features and unique values for categorical features\n",
    "    \"\"\"\n",
    "    bounds = {}\n",
    "    \n",
    "    for feature in feature_names:\n",
    "        if feature not in df.columns:\n",
    "            print(f\"Warning: Feature '{feature}' not found in the dataframe.\")\n",
    "            continue\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "            bounds[feature] = {\n",
    "                'type': 'numeric',\n",
    "                'min': df[feature].min(),\n",
    "                'max': df[feature].max()\n",
    "            }\n",
    "        else:\n",
    "            bounds[feature] = {\n",
    "                'type': 'categorical',\n",
    "                'values': df[feature].unique().tolist()\n",
    "            }\n",
    "    \n",
    "    return bounds\n",
    "\n",
    "# 2. 関数の呼び出し\n",
    "bounds = get_bounds(df_orig, feature_names)\n",
    "\n",
    "# # 3. デバッグ用の関数\n",
    "# def debug_get_bounds():\n",
    "# #     print(\"Current definition of get_bounds:\")\n",
    "# #     print(get_bounds.__code__.co_varnames)\n",
    "# #     print(\"Number of arguments:\", get_bounds.__code__.co_argcount)\n",
    "\n",
    "# # デバッグ関数の呼び出し\n",
    "# debug_get_bounds()\n",
    "\n",
    "# 4. エラーハンドリングを追加した版\n",
    "def get_bounds_safe(df, feature_names):\n",
    "    try:\n",
    "        return [df[feature_names].min().values, df[feature_names].max().values]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_bounds: {e}\")\n",
    "        print(f\"DataFrame columns: {df.columns}\")\n",
    "        print(f\"Requested features: {feature_names}\")\n",
    "        return None\n",
    "\n",
    "# 安全版の関数の呼び出し\n",
    "bounds = get_bounds_safe(df_orig, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '<0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4295/2033319528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Normalize the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4295/1681996337.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(df, target, feature_names, bounds)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_return\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdf_return\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlower_bounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lowprofool/lib/python3.7/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lowprofool/lib/python3.7/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lowprofool/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lowprofool/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m/opt/conda/envs/lowprofool/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNpDtype\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m     def __array_wrap__(\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '<0'"
     ]
    }
   ],
   "source": [
    "# Load initial dataset\n",
    "df_orig, target, feature_names = get_df(DATASET)\n",
    "\n",
    "# Balance dataset classes\n",
    "df = balance_df(df_orig, target)\n",
    "\n",
    "# Compute the bounds for clipping\n",
    "bounds = get_bounds(df_orig, feature_names)\n",
    "\n",
    "# Normalize the data\n",
    "scaler, df, bounds = normalize(df, target, feature_names, bounds)\n",
    "\n",
    "df['target'] = df['target'].astype(float)\n",
    "\n",
    "categorical_features = ['checking_status', 'savings_status', 'employment', 'own_telephone', 'foreign_worker']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_features)\n",
    "\n",
    "# データの分割（エンコード後のデータフレームを使用）\n",
    "df_train_encoded, df_test_encoded, df_valid_encoded = split_train_test_valid(df_encoded)\n",
    "\n",
    "# エンコード後のカラム名から 'target' を除いたリストを使用\n",
    "encoded_feature_names = [col for col in df_encoded.columns if col != 'target']\n",
    "\n",
    "# weightsの計算\n",
    "weights = get_weights(df_encoded, target, encoded_feature_names)\n",
    "\n",
    "# config の更新\n",
    "config = {'Dataset'     : 'credit-g',\n",
    "          'MaxIters'    : 20000, # ハイパーパラメータの設定\n",
    "          'Alpha'       : 0.001, # ハイパーパラメータの設定\n",
    "          'Lambda'      : 8.5, # ハイパーパラメータの設定\n",
    "          'TrainData'   : df_train_encoded,\n",
    "          'TestData'    : df_test_encoded,\n",
    "          'ValidData'   : df_valid_encoded,\n",
    "          'Scaler'      : scaler,\n",
    "          'FeatureNames': encoded_feature_names,  # ここを変更\n",
    "          'Target'      : target,\n",
    "          'Weights'     : weights,\n",
    "          'Bounds'      : bounds}\n",
    "\n",
    "# Train neural network\n",
    "model = get_model(config)\n",
    "config['Model'] = model\n",
    "\n",
    "# Compute accuracy on test set\n",
    "y_true = df_test_encoded[target]\n",
    "x_test = torch.FloatTensor(df_test_encoded[config['FeatureNames']].values)\n",
    "y_pred = model(x_test)\n",
    "y_pred = np.argmax(y_pred.detach().numpy(), axis=1)\n",
    "print(\"Accuracy score on test data\", accuracy_score(y_true, y_pred))\n",
    "\n",
    "# Sub sample\n",
    "config['TestData'] = config['TestData'].sample(n=10, random_state = SEED)\n",
    "\n",
    "# Generate adversarial examples\n",
    "df_adv_lpf = gen_adv(config, 'LowProFool')\n",
    "df_adv_df = gen_adv(config, 'Deepfool')\n",
    "config['AdvData'] = {'LowProFool' : df_adv_lpf, 'Deepfool' : df_adv_df}\n",
    "\n",
    "# Compute metrics\n",
    "list_metrics = {'SuccessRate' : True,\n",
    "                'iter_means': False,\n",
    "                'iter_std': False,\n",
    "                'normdelta_median': False,\n",
    "                'normdelta_mean': True,\n",
    "                'n_std': True,\n",
    "                'weighted_median': False,\n",
    "                'weighted_mean': True,\n",
    "                'w_std': True,\n",
    "                'mean_dists_at_org': False,\n",
    "                'median_dists_at_org': False,\n",
    "                'mean_dists_at_tgt': False,\n",
    "                'mean_dists_at_org_weighted': True,\n",
    "                'mdow_std': True,\n",
    "                'median_dists_at_org_weighted': False,\n",
    "                'mean_dists_at_tgt_weighted': True,\n",
    "                'mdtw_std': True,\n",
    "                'prop_same_class_arg_org': False,\n",
    "                'prop_same_class_arg_adv': False}\n",
    "\n",
    "all_metrics = get_metrics(config, list_metrics)\n",
    "all_metrics = pd.DataFrame(all_metrics, columns=['Method'] + [k for k, v in list_metrics.items() if v])\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4271/4044212574.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplot_ratios\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mm_lpf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LowProFool'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mm_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethod\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'Deepfool'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "plot_ratios = []\n",
    "\n",
    "m_lpf = all_metrics[all_metrics.Method == 'LowProFool']\n",
    "m_df = all_metrics[all_metrics.Method =='Deepfool']\n",
    "\n",
    "sr = m_lpf.SuccessRate.values / m_df.SuccessRate.values \n",
    "wm =  m_lpf.weighted_mean.values / m_df.weighted_mean.values \n",
    "\n",
    "plot_ratios.append([100*sr[0], 100*wm[0]])\n",
    "plot_ratios = pd.DataFrame(plot_ratios, columns=['Success Rate Ratio', 'Mean Perturbation Ratio'])\n",
    "plot_ratios['Dataset'] = 'German Credit'\n",
    "\n",
    "f = plt.figure()\n",
    "ax = plt.axes()\n",
    "plot_ratios.plot(x='Dataset', kind='bar', legend=True, ax=ax)\n",
    "\n",
    "for i, v in enumerate(plot_ratios['Success Rate Ratio'].values):\n",
    "    ax.text(i - 0.2, v - 12 , str(v.round(1)) + '%', fontsize=16, color='white', weight='bold')\n",
    "for i, v in enumerate(plot_ratios['Mean Perturbation Ratio'].values):\n",
    "    ax.text(i + 0.062, v - 12, str(v.round(1)) + '%', fontsize=16, color='white', weight='bold')\n",
    "\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=0, ha='center')\n",
    "ax.axhline(100, ls=':', c='grey')\n",
    "ax.text(-0.49, 100 - 5, '100%')\n",
    "\n",
    "\n",
    "ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
