{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LowProFool\n",
    "## Adversarial examples generation on the german credit dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# Keras \n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "from Adverse import lowProFool, deepfool\n",
    "from Metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAADLCAYAAACyEVKsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAB7CAAAewgFu0HU+AAAEFklEQVR4nO3aMW5jVRiA0TsohWlGNNbIo1hKmcYSTbwFWAw9O2AFLIaaIoUrJO+ByHJHMy4sebqEQEHh8PwpPqd6r7n3L6xPV/f5w+l0Og0AUr659AAA/Js4AwSJM0CQOAMEiTNAkDgDBIkzQJA4AwSJM0CQOAMEiTNAkDgDBIkzQJA4AwSJM0CQOAME3Uy94eFwGNvtdowxxnw+Hzc3k48A8KaOx+PY7/djjDFWq9WYzWZnrzl5Gbfb7Viv11NvCzCJzWYzHh4ezl7HtQZA0OQn5/l8/vLy029jfPw09QhcmT9++fnSI/DO7cZh/DB+H2P8o3FnmDzOr+6YP34a47vPU4/Alfk8vr30CFyRt/qO5loDIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkg6GbqDY/H48vLX7upt+cK/Tm+XHoE3rndODw/v2rcGSaP836/f3n59cept+cKfX/pAbgq+/1+3N3dnb3O5Ncau53TMsB/mfzkfH9///z8+Pg4lsvl1CNwJZ6ensZ6vR5jjLHZbMZisbjwRLxXx+Px+VZgtVq9yZqTx3k2mz0/L5fLcXt7O/UIXKHFYuG3xv/qLa4y/s6/NQCCxBkgSJwBgsQZIEicAYLEGSBInAGCPpxOp9OlhwDgNSdngCBxBggSZ4AgcQYIEmeAIHEGCBJngCBxBggSZ4AgcQYIEmeAIHEGCBJngCBxBggSZ4AgcQYIEmeAIHEGCBJngKCvqKRJBjR40jQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 200x100 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 101,
       "width": 179
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retina display\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "pd.set_option('display.max_columns', 500)\n",
    "tqdm.pandas()\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "ccolors = [\"#008ae9\", \"#ea004f\"]\n",
    "sns.set_palette(ccolors)\n",
    "sns.palplot(sns.color_palette())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "DATASET = 'credit-g'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data from openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(dataset):\n",
    "    assert dataset == 'credit-g', \"This function is specifically for the credit-g dataset\"\n",
    "    \n",
    "    dataset = fetch_openml(dataset, as_frame=True)\n",
    "    df = dataset.data\n",
    "    df['target'] = dataset.target\n",
    "\n",
    "    # Renaming target for training later\n",
    "    df['target'] = df['target'].map({'bad': 0.0, 'good': 1.0})\n",
    "\n",
    "    # Subsetting features to keep only continuous, discrete and ordered categorical\n",
    "    feature_names = ['checking_status', 'duration', 'credit_amount',\n",
    "                     'savings_status', 'employment', 'installment_commitment',\n",
    "                     'residence_since', 'age', 'existing_credits', 'num_dependents',\n",
    "                     'own_telephone', 'foreign_worker']\n",
    "\n",
    "    df = df[feature_names + ['target']]\n",
    "\n",
    "    # Convert categorical variables to numeric\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            # '<0'のような特殊な値を適切に処理\n",
    "            if col in ['checking_status', 'savings_status']:\n",
    "                df[col] = df[col].map({'<0': 0, '0<=X<200': 1, '>=200': 2, 'no checking': 3})\n",
    "            else:\n",
    "                df[col] = pd.Categorical(df[col]).codes\n",
    "\n",
    "    return df, 'target', feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(df, target, feature_names):\n",
    "    if target not in df.columns:\n",
    "        print(f\"Warning: '{target}' column not found. Using all features for correlation.\")\n",
    "        correelation_data = df\n",
    "    else:\n",
    "        correlation_data = df[[target] + [f for f in feature_names if f != target]]\n",
    "    \n",
    "    def heatmap(cor):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "        plt.show()\n",
    "\n",
    "    cor = correlation_data.corr()\n",
    "    cor_target = abs(cor[target]) if target in cor.columns else pd.Series(1, index=feature_names)\n",
    "\n",
    "    weights = cor_target[cor_target.index != target]\n",
    "    weights = weights / np.linalg.norm(weights)\n",
    "    \n",
    "    return weights.values\n",
    "\n",
    "def balance_df(df, target):\n",
    "    len_df_0, len_df_1 = len(df[df[target] == 0]), len(df[df[target] == 1])\n",
    "    min_samples = min(len_df_0, len_df_1)\n",
    "    df_0 = df[df[target] == 0].sample(min_samples, random_state=SEED)\n",
    "    df_1 = df[df[target] == 1].sample(min_samples, random_state=SEED)\n",
    "    return pd.concat((df_0, df_1))\n",
    "\n",
    "def split_train_test_valid(df, test_size=300, valid_size=50):\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, shuffle=True, random_state=SEED)\n",
    "    df_test, df_valid = train_test_split(df_test, test_size=valid_size, shuffle=True, random_state=SEED)\n",
    "    return df_train, df_test, df_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, target, feature_names, bounds):\n",
    "    df_return = df.copy()\n",
    "    numeric_features = [f for f in feature_names if bounds[f]['type'] == 'numeric']\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_return[numeric_features] = scaler.fit_transform(df_return[numeric_features])\n",
    "    \n",
    "    new_bounds = {}\n",
    "    for feature in feature_names:\n",
    "        if bounds[feature]['type'] == 'numeric':\n",
    "            min_val = bounds[feature]['min']\n",
    "            max_val = bounds[feature]['max']\n",
    "            if not np.isnan(min_val) and not np.isnan(max_val):\n",
    "                feature_index = numeric_features.index(feature)\n",
    "                new_min = (min_val - scaler.data_min_[feature_index]) / (scaler.data_max_[feature_index] - scaler.data_min_[feature_index])\n",
    "                new_max = (max_val - scaler.data_min_[feature_index]) / (scaler.data_max_[feature_index] - scaler.data_min_[feature_index])\n",
    "                new_bounds[feature] = {\n",
    "                    'type': 'numeric',\n",
    "                    'min': new_min,\n",
    "                    'max': new_max\n",
    "                }\n",
    "            else:\n",
    "                new_bounds[feature] = {\n",
    "                    'type': 'numeric',\n",
    "                    'min': 0,\n",
    "                    'max': 1\n",
    "                }\n",
    "        else:\n",
    "            new_bounds[feature] = bounds[feature]\n",
    "\n",
    "    return scaler, df_return, new_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(conf, load=False):\n",
    "    assert(conf['Dataset'] == 'credit-g')\n",
    "    \n",
    "    class GermanNet(nn.Module):\n",
    "        def __init__(self, D_in, H, D_out):\n",
    "            super(GermanNet, self).__init__()\n",
    "            self.linear1 = torch.nn.Linear(D_in, H)\n",
    "            self.linear2 = torch.nn.Linear(H, H)\n",
    "            self.linear3 = torch.nn.Linear(H, D_out)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.softmax = torch.nn.Softmax(dim=0) # softmax関数の次元の修正 行方向に適用するため1\n",
    "\n",
    "        def forward(self, x):\n",
    "            h1 = self.relu(self.linear1(x))\n",
    "            h2 = self.relu(self.linear2(h1))\n",
    "            h3 = self.relu(self.linear2(h2))\n",
    "            h4 = self.relu(self.linear2(h3))\n",
    "            h5 = self.relu(self.linear2(h4))\n",
    "            h6 = self.relu(self.linear2(h5))\n",
    "            return self.linear3(h6).view(-1, 1)  # 出力を [batch_size, 1] の形状に変更\n",
    "\n",
    "    def train(model, criterion, optimizer, X, y, N):\n",
    "        model.train()\n",
    "\n",
    "        current_loss = 0\n",
    "        current_correct = 0\n",
    "\n",
    "\n",
    "        # Training in batches\n",
    "        for ind in range(0, X.size(0), N):\n",
    "            indices = range(ind, min(ind + N, X.size(0))) \n",
    "            inputs, labels = X[indices], y[indices]\n",
    "            inputs = Variable(inputs, requires_grad=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(inputs)\n",
    "            print(\"Output shape:\", output.shape)\n",
    "            print(\"Labels shape:\", labels.shape)\n",
    "            print(\"Output sample:\", output[0])\n",
    "            print(\"Labels sample:\", labels[0])\n",
    "\n",
    "            assert output.shape == labels.shape, f\"Output shape{output.shape} doesn't match labels shape {labels.shape}\"\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            print(\"Loss:\", loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss += loss.item()\n",
    "            preds = (torch.sigmoid(output) > 0.5).float()\n",
    "            current_correct += (preds == labels).float().sum()\n",
    "\n",
    "        current_loss = current_loss / (X.size(0) // N)\n",
    "        current_correct = current_correct.item() / X.size(0)    \n",
    "\n",
    "        return current_loss, current_correct\n",
    "    \n",
    "    df = conf['TrainData']\n",
    "    target = conf['Target']\n",
    "    feature_names = conf['FeatureNames']\n",
    "                        \n",
    "    # n_classes = len(np.unique(df[target]))\n",
    "    X_train = torch.FloatTensor(df[conf['FeatureNames']].values) # この行はそのままでOK\n",
    "    # y_train = keras.utils.to_categorical(df[target], n_classes)\n",
    "    y_train = torch.FloatTensor(df[target].values).unsqueeze(1)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "    D_in = X_train.size(1)\n",
    "    D_out = 1\n",
    "\n",
    "    epochs = 400\n",
    "    batch_size = 100\n",
    "    H = 100\n",
    "    net = GermanNet(D_in, H, D_out)\n",
    "\n",
    "    lr = 1e-3\n",
    "    criterion = torch.nn.BCEWithLogitsLoss() # BCELossから変更\n",
    "    # criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss, epoch_acc = train(net, criterion, optimizer, X_train, y_train, batch_size)     \n",
    "        if (epoch % 50 == 0):\n",
    "            print(\"> epoch {:.0f}\\tLoss {:.5f}\\tAcc {:.5f}\".format(epoch, epoch_loss, epoch_acc))\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv(config, method):\n",
    "    df_test = config['TestData']\n",
    "    feature_names = config['FeatureNames']\n",
    "    extra_cols = ['orig_pred', 'adv_pred', 'iters']    \n",
    "    model = config['Model']\n",
    "    weights = config['Weights']\n",
    "    bounds = config['Bounds']\n",
    "\n",
    "    encoded_bounds = {}\n",
    "    for feature, info in bounds.items():\n",
    "        if info['type'] == 'numeric':\n",
    "            encoded_bounds[feature] = info\n",
    "        elif info['type'] == 'categorical':\n",
    "            for value in info['values']:\n",
    "                encoded_bounds[f\"{feature}_{value}\"] = {'type': 'numeric', 'min':0, 'max':1}\n",
    "\n",
    "    maxiters = config['MaxIters']\n",
    "    alpha = config['Alpha']\n",
    "    lambda_ = config['Lambda']\n",
    "    \n",
    "    results = np.zeros((len(df_test), len(feature_names) + len(extra_cols)))\n",
    "\n",
    "    i = -1\n",
    "    for _, row in tqdm_notebook(df_test.iterrows(), total=df_test.shape[0], desc=\"{}\".format(method)):\n",
    "        i += 1\n",
    "        x_tensor = torch.FloatTensor(row[config['FeatureNames']])\n",
    "\n",
    "        # print(\"Shape of x_tensor:\", x_tensor.shape)\n",
    "        \n",
    "        if method == 'LowProFool':\n",
    "            orig_pred, adv_pred, x_adv, loop_i = lowProFool(x_tensor, model, weights, encoded_bounds, maxiters, alpha, lambda_)\n",
    "        elif method == 'Deepfool':\n",
    "            orig_pred, adv_pred, x_adv, loop_i = deepfool(x_tensor, model, maxiters, alpha, encoded_bounds, weights=[])\n",
    "        else:\n",
    "            raise Exception(\"Invalid method\", method)\n",
    "    \n",
    "        # x_advがTensorの場合、NumPy配列に変換\n",
    "        if isinstance(x_adv, torch.Tensor):\n",
    "            x_adv = x_adv.cpu().numpy()\n",
    "\n",
    "        # x_advを1次元に平坦化\n",
    "        x_adv_flat = x_adv.flatten()\n",
    "\n",
    "        # 結果を結合\n",
    "        results[i] = np.concatenate((x_adv_flat, [orig_pred, adv_pred, loop_i]))\n",
    "\n",
    "        if i == 0: # 最初の反復でのみ結果を表示\n",
    "            print(\"First row of results:\", results[0])\n",
    "            # break;\n",
    "        \n",
    "    return pd.DataFrame(results, index=df_test.index, columns = feature_names + extra_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_preprocess_data(df, feature_names):\n",
    "    df = df.copy()\n",
    "    for col in feature_names:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = pd.Categorical(df[col]).codes\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # NaN値を各列の中央値で埋める\n",
    "    df = df.fillna(df.median())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_categorical(df, feature_names):\n",
    "    le = LabelEncoder()\n",
    "    for col in feature_names:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "    return df\n",
    "\n",
    "# df = encode_categorical(df, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/sklearn/datasets/_openml.py:421: UserWarning: Multiple active versions of the dataset matching the name credit-g exist. Versions may be fundamentally different, returning version 1.\n",
      "  \" {version}.\".format(name=name, version=res[0][\"version\"])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Categorical is not ordered for operation min\nyou can use .as_ordered() to change the Categorical to an ordered one\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5152/632770941.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# 2. 関数の呼び出し\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mbounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# # 3. デバッグ用の関数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5152/632770941.py\u001b[0m in \u001b[0;36mget_bounds\u001b[0;34m(df, feature_names)\u001b[0m\n\u001b[1;32m     22\u001b[0m         bounds[feature] = {\n\u001b[1;32m     23\u001b[0m             \u001b[0;34m'type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'numeric'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;34m'min'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;34m'max'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         }\n",
      "\u001b[0;32m/opt/conda/envs/lowprofool/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mmin\u001b[0;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  10834\u001b[0m         )\n\u001b[1;32m  10835\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10836\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10838\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"min\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lowprofool/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mmin\u001b[0;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  10358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10359\u001b[0m         return self._stat_function(\n\u001b[0;32m> 10360\u001b[0;31m             \u001b[0;34m\"min\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnanops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10361\u001b[0m         )\n\u001b[1;32m  10362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lowprofool/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_stat_function\u001b[0;34m(self, name, func, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  10353\u001b[0m             )\n\u001b[1;32m  10354\u001b[0m         return self._reduce(\n\u001b[0;32m> 10355\u001b[0;31m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10356\u001b[0m         )\n\u001b[1;32m  10357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lowprofool/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   4381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtensionArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4382\u001b[0m             \u001b[0;31m# dispatch to ExtensionArray interface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4383\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdelegate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lowprofool/lib/python3.7/site-packages/pandas/core/arrays/_mixins.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, name, skipna, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"'{type(self).__name__}' does not implement reduction '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lowprofool/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lowprofool/lib/python3.7/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36mmin\u001b[0;34m(self, skipna, **kwargs)\u001b[0m\n\u001b[1;32m   2109\u001b[0m         \u001b[0mnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_minmax_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2110\u001b[0m         \u001b[0mnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_for_ordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_codes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/lowprofool/lib/python3.7/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36mcheck_for_ordered\u001b[0;34m(self, op)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mordered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m             raise TypeError(\n\u001b[0;32m-> 1647\u001b[0;31m                 \u001b[0;34mf\"Categorical is not ordered for operation {op}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m                 \u001b[0;34m\"you can use .as_ordered() to change the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m                 \u001b[0;34m\"Categorical to an ordered one\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Categorical is not ordered for operation min\nyou can use .as_ordered() to change the Categorical to an ordered one\n"
     ]
    }
   ],
   "source": [
    "# Load initial dataset\n",
    "df_orig, target, feature_names = get_df(DATASET)\n",
    "\n",
    "# Balance dataset classes\n",
    "df = balance_df(df_orig, target)\n",
    "\n",
    "# 1. 正しい関数定義\n",
    "def get_bounds(df, feature_names):\n",
    "    \"\"\"\n",
    "    Calculate the bounds for given features in a dataframe, handling both numeric and categorical variables.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        feature_names (list): List of feature names to calculate bounds for\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing bounds for numeric features and unique values for categorical features\n",
    "    \"\"\"\n",
    "\n",
    "    bounds = {}\n",
    "    for feature in feature_names:\n",
    "        bounds[feature] = {\n",
    "            'type': 'numeric',\n",
    "            'min': df[feature].min(),\n",
    "            'max': df[feature].max()\n",
    "        }\n",
    "    return bounds\n",
    "\n",
    "# 2. 関数の呼び出し\n",
    "bounds = get_bounds(df_orig, feature_names)\n",
    "\n",
    "# # 3. デバッグ用の関数\n",
    "# def debug_get_bounds():\n",
    "# #     print(\"Current definition of get_bounds:\")\n",
    "# #     print(get_bounds.__code__.co_varnames)\n",
    "# #     print(\"Number of arguments:\", get_bounds.__code__.co_argcount)\n",
    "\n",
    "# # デバッグ関数の呼び出し\n",
    "# debug_get_bounds()\n",
    "\n",
    "# 4. エラーハンドリングを追加した版\n",
    "def get_bounds_safe(df, feature_names):\n",
    "    try:\n",
    "        return [df[feature_names].min().values, df[feature_names].max().values]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_bounds: {e}\")\n",
    "        print(f\"DataFrame columns: {df.columns}\")\n",
    "        print(f\"Requested features: {feature_names}\")\n",
    "        return None\n",
    "\n",
    "# 安全版の関数の呼び出し\n",
    "bounds = get_bounds_safe(df_orig, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1117: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1117: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1117: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1117: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1117: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/sklearn/preprocessing/_data.py:461: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/sklearn/preprocessing/_data.py:462: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounds: {'checking_status': {'type': 'numeric', 'min': nan, 'max': nan}, 'duration': {'type': 'numeric', 'min': 4.0, 'max': 72.0}, 'credit_amount': {'type': 'numeric', 'min': 250.0, 'max': 18424.0}, 'savings_status': {'type': 'numeric', 'min': nan, 'max': nan}, 'employment': {'type': 'numeric', 'min': nan, 'max': nan}, 'installment_commitment': {'type': 'numeric', 'min': 1.0, 'max': 4.0}, 'residence_since': {'type': 'numeric', 'min': 1.0, 'max': 4.0}, 'age': {'type': 'numeric', 'min': 19.0, 'max': 74.0}, 'existing_credits': {'type': 'numeric', 'min': 1.0, 'max': 4.0}, 'num_dependents': {'type': 'numeric', 'min': 1.0, 'max': 2.0}, 'own_telephone': {'type': 'numeric', 'min': nan, 'max': nan}, 'foreign_worker': {'type': 'numeric', 'min': nan, 'max': nan}}\n",
      "Index(['checking_status', 'duration', 'credit_amount', 'savings_status',\n",
      "       'employment', 'installment_commitment', 'residence_since', 'age',\n",
      "       'existing_credits', 'num_dependents', 'own_telephone', 'foreign_worker',\n",
      "       'target'],\n",
      "      dtype='object')\n",
      "Target column present: True\n",
      "y_train shape: torch.Size([300, 1])\n",
      "> epoch 0\tLoss nan\tAcc 0.51667\n",
      "> epoch 50\tLoss nan\tAcc 0.51667\n",
      "> epoch 100\tLoss nan\tAcc 0.51667\n",
      "> epoch 150\tLoss nan\tAcc 0.51667\n",
      "> epoch 200\tLoss nan\tAcc 0.51667\n",
      "> epoch 250\tLoss nan\tAcc 0.51667\n",
      "> epoch 300\tLoss nan\tAcc 0.51667\n",
      "> epoch 350\tLoss nan\tAcc 0.51667\n",
      "Accuracy score on test data 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/ipykernel_launcher.py:24: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004052981b994959a9f08da22bc6a970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LowProFool:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "can't assign a numpy.ndarray to a torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4887/1759801766.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Generate adversarial examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mdf_adv_lpf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_adv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LowProFool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0mdf_adv_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_adv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Deepfool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AdvData'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'LowProFool'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdf_adv_lpf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Deepfool'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdf_adv_df\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4887/931074850.py\u001b[0m in \u001b[0;36mgen_adv\u001b[0;34m(config, method)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LowProFool'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0morig_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlowProFool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_bounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Deepfool'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0morig_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepfool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_bounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/Adverse.py\u001b[0m in \u001b[0;36mlowProFool\u001b[0;34m(x, model, weights, bounds, maxiters, alpha, lambda_)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mlambda_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't assign a numpy.ndarray to a torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "df_orig, target, feature_names = get_df(DATASET)\n",
    "\n",
    "# データセットのバランス\n",
    "df = balance_df(df_orig, target)\n",
    "\n",
    "# データのクリーニングと前処理\n",
    "df = clean_and_preprocess_data(df, feature_names)\n",
    "\n",
    "# デバッグ\n",
    "# print(\"Data types:\")\n",
    "# print(df[feature_names].dtypes)\n",
    "# print(\"\\nNull values:\")\n",
    "# print(df[feature_names].isnull().sum())\n",
    "# print(\"\\nUnique values in object columns:\")\n",
    "# print(df[feature_names].select_dtypes(include=['object']).nunique())\n",
    "\n",
    "# カテゴリカル変数の特定\n",
    "categorical_features = df[feature_names].select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# カテゴリカル変数のOne-Hotエンコーディング\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_features)\n",
    "\n",
    "# エンコード後のカラム名から 'target' を除いたリストを使用\n",
    "encoded_feature_names = [col for col in df_encoded.columns if col != 'target']\n",
    "\n",
    "# バウンドの計算\n",
    "bounds = get_bounds(df_encoded, encoded_feature_names)\n",
    "\n",
    "# デバッグ\n",
    "print(\"Bounds:\", bounds)\n",
    "\n",
    "# データの正規化\n",
    "scaler, df_normalized, bounds = normalize(df_encoded, target, encoded_feature_names, bounds)\n",
    "\n",
    "# デバッグ\n",
    "print(df_normalized.columns)\n",
    "print(\"Target column present:\", target in df_normalized.columns)\n",
    "\n",
    "# データの分割（エンコード後のデータフレームを使用）\n",
    "df_train, df_test, df_valid = split_train_test_valid(df_normalized)\n",
    "\n",
    "# weightsの計算\n",
    "weights = get_weights(df_normalized, target, encoded_feature_names)\n",
    "\n",
    "# config の更新\n",
    "config = {'Dataset'     : 'credit-g',\n",
    "          'MaxIters'    : 20000, # ハイパーパラメータの設定\n",
    "          'Alpha'       : 0.001, # ハイパーパラメータの設定\n",
    "          'Lambda'      : 8.5, # ハイパーパラメータの設定\n",
    "          'TrainData'   : df_train,\n",
    "          'TestData'    : df_test,\n",
    "          'ValidData'   : df_valid,\n",
    "          'Scaler'      : scaler,\n",
    "          'FeatureNames': encoded_feature_names,\n",
    "          'Target'      : target,\n",
    "          'Weights'     : weights,\n",
    "          'Bounds'      : bounds}\n",
    "\n",
    "# Train neural network\n",
    "model = get_model(config)\n",
    "config['Model'] = model\n",
    "\n",
    "# Compute accuracy on test set\n",
    "y_true = df_test[target]\n",
    "x_test = torch.FloatTensor(df_test[config['FeatureNames']].values)\n",
    "y_pred = model(x_test)\n",
    "y_pred = (torch.sigmoid(y_pred) > 0.5).float().numpy()\n",
    "print(\"Accuracy score on test data\", accuracy_score(y_true, y_pred))\n",
    "\n",
    "# Sub sample\n",
    "config['TestData'] = config['TestData'].sample(n=10, random_state = SEED)\n",
    "\n",
    "# Generate adversarial examples\n",
    "df_adv_lpf = gen_adv(config, 'LowProFool')\n",
    "df_adv_df = gen_adv(config, 'Deepfool')\n",
    "config['AdvData'] = {'LowProFool' : df_adv_lpf, 'Deepfool' : df_adv_df}\n",
    "\n",
    "# # Compute metrics\n",
    "# list_metrics = {'SuccessRate' : True,\n",
    "#                 'iter_means': False,\n",
    "#                 'iter_std': False,\n",
    "#                 'normdelta_median': False,\n",
    "#                 'normdelta_mean': True,\n",
    "#                 'n_std': True,\n",
    "#                 'weighted_median': False,\n",
    "#                 'weighted_mean': True,\n",
    "#                 'w_std': True,\n",
    "#                 'mean_dists_at_org': False,\n",
    "#                 'median_dists_at_org': False,\n",
    "#                 'mean_dists_at_tgt': False,\n",
    "#                 'mean_dists_at_org_weighted': True,\n",
    "#                 'mdow_std': True,\n",
    "#                 'median_dists_at_org_weighted': False,\n",
    "#                 'mean_dists_at_tgt_weighted': True,\n",
    "#                 'mdtw_std': True,\n",
    "#                 'prop_same_class_arg_org': False,\n",
    "#                 'prop_same_class_arg_adv': False}\n",
    "\n",
    "# メトリクスの計算\n",
    "list_metrics = {\n",
    "    'SuccessRate': True, 'normdelta_mean': True, 'n_std': True,\n",
    "    'weighted_mean': True, 'w_std': True,\n",
    "    'mean_dists_at_org_weighted': True, 'mdow_std': True,\n",
    "    'mean_dists_at_tgt_weighted': True, 'mdtw_std': True\n",
    "}\n",
    "\n",
    "all_metrics = get_metrics(config, list_metrics)\n",
    "all_metrics = pd.DataFrame(all_metrics, columns=['Method'] + [k for k, v in list_metrics.items() if v])\n",
    "print(\"all_metrics:\",all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4271/4044212574.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplot_ratios\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mm_lpf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LowProFool'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mm_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethod\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'Deepfool'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "plot_ratios = []\n",
    "\n",
    "m_lpf = all_metrics[all_metrics.Method == 'LowProFool']\n",
    "m_df = all_metrics[all_metrics.Method =='Deepfool']\n",
    "\n",
    "sr = m_lpf.SuccessRate.values / m_df.SuccessRate.values \n",
    "wm =  m_lpf.weighted_mean.values / m_df.weighted_mean.values \n",
    "\n",
    "plot_ratios.append([100*sr[0], 100*wm[0]])\n",
    "plot_ratios = pd.DataFrame(plot_ratios, columns=['Success Rate Ratio', 'Mean Perturbation Ratio'])\n",
    "plot_ratios['Dataset'] = 'German Credit'\n",
    "\n",
    "f = plt.figure()\n",
    "ax = plt.axes()\n",
    "plot_ratios.plot(x='Dataset', kind='bar', legend=True, ax=ax)\n",
    "\n",
    "for i, v in enumerate(plot_ratios['Success Rate Ratio'].values):\n",
    "    ax.text(i - 0.2, v - 12 , str(v.round(1)) + '%', fontsize=16, color='white', weight='bold')\n",
    "for i, v in enumerate(plot_ratios['Mean Perturbation Ratio'].values):\n",
    "    ax.text(i + 0.062, v - 12, str(v.round(1)) + '%', fontsize=16, color='white', weight='bold')\n",
    "\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=0, ha='center')\n",
    "ax.axhline(100, ls=':', c='grey')\n",
    "ax.text(-0.49, 100 - 5, '100%')\n",
    "\n",
    "\n",
    "ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
