{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LowProFool\n",
    "## Adversarial examples generation on the german credit dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# Keras \n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "from Adverse import lowProFool, deepfool\n",
    "from Metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAADLCAYAAACyEVKsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAB7CAAAewgFu0HU+AAAEFklEQVR4nO3aMW5jVRiA0TsohWlGNNbIo1hKmcYSTbwFWAw9O2AFLIaaIoUrJO+ByHJHMy4sebqEQEHh8PwpPqd6r7n3L6xPV/f5w+l0Og0AUr659AAA/Js4AwSJM0CQOAMEiTNAkDgDBIkzQJA4AwSJM0CQOAMEiTNAkDgDBIkzQJA4AwSJM0CQOAME3Uy94eFwGNvtdowxxnw+Hzc3k48A8KaOx+PY7/djjDFWq9WYzWZnrzl5Gbfb7Viv11NvCzCJzWYzHh4ezl7HtQZA0OQn5/l8/vLy029jfPw09QhcmT9++fnSI/DO7cZh/DB+H2P8o3FnmDzOr+6YP34a47vPU4/Alfk8vr30CFyRt/qO5loDIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkgSJwBgsQZIEicAYLEGSBInAGCxBkg6GbqDY/H48vLX7upt+cK/Tm+XHoE3rndODw/v2rcGSaP836/f3n59cept+cKfX/pAbgq+/1+3N3dnb3O5Ncau53TMsB/mfzkfH9///z8+Pg4lsvl1CNwJZ6ensZ6vR5jjLHZbMZisbjwRLxXx+Px+VZgtVq9yZqTx3k2mz0/L5fLcXt7O/UIXKHFYuG3xv/qLa4y/s6/NQCCxBkgSJwBgsQZIEicAYLEGSBInAGCPpxOp9OlhwDgNSdngCBxBggSZ4AgcQYIEmeAIHEGCBJngCBxBggSZ4AgcQYIEmeAIHEGCBJngCBxBggSZ4AgcQYIEmeAIHEGCBJngKCvqKRJBjR40jQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 200x100 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 101,
       "width": 179
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retina display\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "pd.set_option('display.max_columns', 500)\n",
    "tqdm.pandas()\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "ccolors = [\"#008ae9\", \"#ea004f\"]\n",
    "sns.set_palette(ccolors)\n",
    "sns.palplot(sns.color_palette())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "DATASET = 'credit-g'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data from openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(dataset):\n",
    "    assert dataset == 'credit-g', \"This function is specifically for the credit-g dataset\"\n",
    "    \n",
    "    dataset = fetch_openml(dataset, as_frame=True)\n",
    "    df = dataset.data\n",
    "    df['target'] = dataset.target\n",
    "\n",
    "    # Renaming target for training later\n",
    "    df['target'] = df['target'].map({'bad': 0.0, 'good': 1.0})\n",
    "\n",
    "    # Subsetting features to keep only continuous, discrete and ordered categorical\n",
    "    feature_names = ['checking_status', 'duration', 'credit_amount',\n",
    "                     'savings_status', 'employment', 'installment_commitment',\n",
    "                     'residence_since', 'age', 'existing_credits', 'num_dependents',\n",
    "                     'own_telephone', 'foreign_worker']\n",
    "\n",
    "    df = df[feature_names + ['target']]\n",
    "\n",
    "    # Convert categorical variables to numeric\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = pd.Categorical(df[col]).codes\n",
    "\n",
    "    return df, 'target', feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(df, target, show_heatmap=False):\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"Column '{target}' not found in dataframe. Available columns: {df.columns}\")\n",
    "\n",
    "    def heatmap(cor):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "        plt.show()\n",
    "\n",
    "    cor = df.corr()\n",
    "    cor_target = abs(cor[target])\n",
    "\n",
    "    weights = cor_target[cor_target.index != target]\n",
    "    weights = weights / np.linalg.norm(weights)\n",
    "\n",
    "    if show_heatmap:\n",
    "        heatmap(cor)\n",
    "            \n",
    "    return weights.values\n",
    "\n",
    "def balance_df(df, target):\n",
    "    len_df_0, len_df_1 = len(df[df[target] == 0]), len(df[df[target] == 1])\n",
    "    min_samples = min(len_df_0, len_df_1)\n",
    "    df_0 = df[df[target] == 0].sample(min_samples, random_state=SEED)\n",
    "    df_1 = df[df[target] == 1].sample(min_samples, random_state=SEED)\n",
    "    return pd.concat((df_0, df_1))\n",
    "\n",
    "def get_bounds(df, feature_names):\n",
    "    return [df[feature_names].min().values, df[feature_names].max().values]\n",
    "# def get_bounds():\n",
    "#     low_bounds = df_orig.min().values\n",
    "#     up_bounds = df_orig.max().values\n",
    "    \n",
    "#     #removing target WARNING ASSUMES TARGET IS LAST\n",
    "#     low_bounds = low_bounds[:-1]\n",
    "#     up_bounds = up_bounds[:-1]\n",
    "    \n",
    "#     return [low_bounds, up_bounds]\n",
    "\n",
    "def split_train_test_valid(df, test_size=300, valid_size=50):\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, shuffle=True, random_state=SEED)\n",
    "    df_test, df_valid = train_test_split(df_test, test_size=valid_size, shuffle=True, random_state=SEED)\n",
    "    return df_train, df_test, df_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, target, feature_names, bounds):\n",
    "    # print(\"Input bounds:\", bounds)\n",
    "    # print(\"Feature names:\", feature_names)\n",
    "    \n",
    "    # boundsがリスト形式の場合の処理\n",
    "    if isinstance(bounds, list) and len(bounds) == 2:\n",
    "        min_bounds, max_bounds = bounds\n",
    "        numeric_features = [f for f in feature_names if pd.api.types.is_numeric_dtype(df[f])]\n",
    "        bounds_dict = {\n",
    "            feature: {'type': 'numeric', 'min': min_val, 'max': max_val}\n",
    "            for feature, min_val, max_val in zip(numeric_features, min_bounds, max_bounds)\n",
    "        }\n",
    "        # カテゴリカル特徴量に対するboundsを追加\n",
    "        for feature in feature_names:\n",
    "            if feature not in bounds_dict:\n",
    "                bounds_dict[feature] = {'type': 'categorical', 'values': df[feature].unique().tolist()}\n",
    "    else:\n",
    "        bounds_dict = bounds  # 既存の辞書形式の場合\n",
    "    \n",
    "    # print(\"Bounds dict:\", bounds_dict)\n",
    "\n",
    "    # 数値特徴量とカテゴリカル特徴量を分離\n",
    "    numeric_features = [f for f in feature_names if pd.api.types.is_numeric_dtype(df[f])]\n",
    "    categorical_features = [f for f in feature_names if f not in numeric_features]\n",
    "\n",
    "    # print(\"Numeric features:\", numeric_features)\n",
    "    print(\"Categorical features:\", categorical_features)\n",
    "\n",
    "    # スケーリング\n",
    "    scaler = StandardScaler()\n",
    "    if numeric_features:\n",
    "        df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
    "\n",
    "    # boundsの更新\n",
    "    scaled_bounds = {}\n",
    "    if numeric_features:\n",
    "        original_mins = pd.DataFrame({f: [bounds_dict[f]['min']] for f in numeric_features})\n",
    "        original_maxs = pd.DataFrame({f: [bounds_dict[f]['max']] for f in numeric_features})\n",
    "        scaled_mins = scaler.transform(original_mins)\n",
    "        scaled_maxs = scaler.transform(original_maxs)\n",
    "        \n",
    "        for feature, scaled_min, scaled_max in zip(numeric_features, scaled_mins[0], scaled_maxs[0]):\n",
    "            scaled_bounds[feature] = {'type': 'numeric', 'min': scaled_min, 'max': scaled_max}\n",
    "\n",
    "    # カテゴリカル特徴量の処理（ここではそのまま保持）\n",
    "    for feature in categorical_features:\n",
    "        scaled_bounds[feature] = bounds_dict[feature]\n",
    "\n",
    "    return scaler, df, scaled_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(conf, load=False):\n",
    "    assert(conf['Dataset'] == 'credit-g')\n",
    "    \n",
    "    class GermanNet(nn.Module):\n",
    "        def __init__(self, D_in, H, D_out):\n",
    "            super(GermanNet, self).__init__()\n",
    "            self.linear1 = torch.nn.Linear(D_in, H)\n",
    "            self.linear2 = torch.nn.Linear(H, H)\n",
    "            self.linear3 = torch.nn.Linear(H, D_out)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.softmax = torch.nn.Softmax(dim=0)\n",
    "\n",
    "        def forward(self, x):\n",
    "            h1 = self.relu(self.linear1(x))\n",
    "            h2 = self.relu(self.linear2(h1))\n",
    "            h3 = self.relu(self.linear2(h2))\n",
    "            h4 = self.relu(self.linear2(h3))\n",
    "            h5 = self.relu(self.linear2(h4))\n",
    "            h6 = self.relu(self.linear2(h5))\n",
    "            a3 = self.linear3(h6)\n",
    "            y = self.softmax(a3)\n",
    "            return y\n",
    "\n",
    "    def train(model, criterion, optimizer, X, y, N, n_classes):\n",
    "        model.train()\n",
    "\n",
    "        current_loss = 0\n",
    "        current_correct = 0\n",
    "\n",
    "\n",
    "        # Training in batches\n",
    "        for ind in range(0, X.size(0), N):\n",
    "            indices = range(ind, min(ind + N, X.size(0)) - 1) \n",
    "            inputs, labels = X[indices], y[indices]\n",
    "            inputs = Variable(inputs, requires_grad=True)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(inputs)\n",
    "            _, indices = torch.max(output, 1) # argmax of output [[0.61, 0.12]] -> [0]\n",
    "            # [[0, 1, 1, 0, 1, 0, 0]] -> [[1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0]]\n",
    "            preds = torch.tensor(keras.utils.to_categorical(indices, num_classes=n_classes))\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss += loss.item()\n",
    "            current_correct += (preds.int() == labels.int()).sum() /n_classes\n",
    "\n",
    "\n",
    "        current_loss = current_loss / X.size(0)\n",
    "        current_correct = current_correct.double() / X.size(0)    \n",
    "\n",
    "        return preds, current_loss, current_correct.item()\n",
    "    \n",
    "    df = conf['TrainData']\n",
    "    target = conf['Target']\n",
    "    feature_names = conf['FeatureNames']\n",
    "                        \n",
    "    n_classes = len(np.unique(df[target]))\n",
    "    X_train = torch.FloatTensor(df[feature_names].values)\n",
    "    y_train = keras.utils.to_categorical(df[target], n_classes)\n",
    "    y_train = torch.FloatTensor(y_train)\n",
    "\n",
    "    D_in = X_train.size(1)\n",
    "    D_out = y_train.size(1)\n",
    "\n",
    "    epochs = 400\n",
    "    batch_size = 100\n",
    "    H = 100\n",
    "    net = GermanNet(D_in, H, D_out)\n",
    "\n",
    "    lr = 1e-4    \n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        preds, epoch_loss, epoch_acc = train(net, criterion, optimizer, X_train, y_train, batch_size, n_classes)     \n",
    "        if (epoch % 50 == 0):\n",
    "            print(\"> epoch {:.0f}\\tLoss {:.5f}\\tAcc {:.5f}\".format(epoch, epoch_loss, epoch_acc))\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv(config, method):\n",
    "    df_test = config['TestData']\n",
    "    extra_cols = ['orig_pred', 'adv_pred', 'iters']    \n",
    "    model = config['Model']\n",
    "    weights = config['Weights']\n",
    "    bounds = config['Bounds']\n",
    "    maxiters = config['MaxIters']\n",
    "    alpha = config['Alpha']\n",
    "    lambda_ = config['Lambda']\n",
    "    \n",
    "    results = np.zeros((len(df_test), len(feature_names) + len(extra_cols)))    \n",
    "            \n",
    "    i = -1\n",
    "    for _, row in tqdm_notebook(df_test.iterrows(), total=df_test.shape[0], desc=\"{}\".format(method)):\n",
    "        i += 1\n",
    "        x_tensor = torch.FloatTensor(row[config['FeatureNames']])   \n",
    "        \n",
    "        if method == 'LowProFool':\n",
    "            orig_pred, adv_pred, x_adv, loop_i = lowProFool(x_tensor, model, weights, bounds,\n",
    "                                                             maxiters, alpha, lambda_)\n",
    "        elif method == 'Deepfool':\n",
    "            orig_pred, adv_pred, x_adv, loop_i = deepfool(x_tensor, model, maxiters, alpha,\n",
    "                                                          bounds, weights=[])\n",
    "        else:\n",
    "            raise Exception(\"Invalid method\", method)\n",
    "        results[i] = np.concatenate((x_adv, [orig_pred, adv_pred, loop_i]), axis=0)\n",
    "        \n",
    "    return pd.DataFrame(results, index=df_test.index, columns = feature_names + extra_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current definition of get_bounds:\n",
      "('df', 'feature_names', 'bounds', 'feature')\n",
      "Number of arguments: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/sklearn/datasets/_openml.py:421: UserWarning: Multiple active versions of the dataset matching the name credit-g exist. Versions may be fundamentally different, returning version 1.\n",
      "  \" {version}.\".format(name=name, version=res[0][\"version\"])\n",
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/ipykernel_launcher.py:55: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n"
     ]
    }
   ],
   "source": [
    "# Load initial dataset\n",
    "df_orig, target, feature_names = get_df(DATASET)\n",
    "\n",
    "# Balance dataset classes\n",
    "df = balance_df(df_orig, target)\n",
    "\n",
    "# 1. 正しい関数定義\n",
    "def get_bounds(df, feature_names):\n",
    "    \"\"\"\n",
    "    Calculate the bounds for given features in a dataframe, handling both numeric and categorical variables.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        feature_names (list): List of feature names to calculate bounds for\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing bounds for numeric features and unique values for categorical features\n",
    "    \"\"\"\n",
    "    bounds = {}\n",
    "    \n",
    "    for feature in feature_names:\n",
    "        if feature not in df.columns:\n",
    "            print(f\"Warning: Feature '{feature}' not found in the dataframe.\")\n",
    "            continue\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "            bounds[feature] = {\n",
    "                'type': 'numeric',\n",
    "                'min': df[feature].min(),\n",
    "                'max': df[feature].max()\n",
    "            }\n",
    "        else:\n",
    "            bounds[feature] = {\n",
    "                'type': 'categorical',\n",
    "                'values': df[feature].unique().tolist()\n",
    "            }\n",
    "    \n",
    "    return bounds\n",
    "\n",
    "# 2. 関数の呼び出し\n",
    "bounds = get_bounds(df_orig, feature_names)\n",
    "\n",
    "# 3. デバッグ用の関数\n",
    "def debug_get_bounds():\n",
    "    print(\"Current definition of get_bounds:\")\n",
    "    print(get_bounds.__code__.co_varnames)\n",
    "    print(\"Number of arguments:\", get_bounds.__code__.co_argcount)\n",
    "\n",
    "# デバッグ関数の呼び出し\n",
    "debug_get_bounds()\n",
    "\n",
    "# 4. エラーハンドリングを追加した版\n",
    "def get_bounds_safe(df, feature_names):\n",
    "    try:\n",
    "        return [df[feature_names].min().values, df[feature_names].max().values]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_bounds: {e}\")\n",
    "        print(f\"DataFrame columns: {df.columns}\")\n",
    "        print(f\"Requested features: {feature_names}\")\n",
    "        return None\n",
    "\n",
    "# 安全版の関数の呼び出し\n",
    "bounds = get_bounds_safe(df_orig, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: ['checking_status', 'savings_status', 'employment', 'own_telephone', 'foreign_worker']\n",
      "Target value: target\n",
      "DataFrame columns: Index(['checking_status', 'duration', 'credit_amount', 'savings_status',\n",
      "       'employment', 'installment_commitment', 'residence_since', 'age',\n",
      "       'existing_credits', 'num_dependents', 'own_telephone', 'foreign_worker',\n",
      "       'target'],\n",
      "      dtype='object')\n",
      "Is target in columns: True\n",
      "float64\n",
      "0.0    300\n",
      "1.0    300\n",
      "Name: target, dtype: int64\n",
      "DataFrame shape: (600, 13)\n",
      "DataFrame columns: Index(['checking_status', 'duration', 'credit_amount', 'savings_status',\n",
      "       'employment', 'installment_commitment', 'residence_since', 'age',\n",
      "       'existing_credits', 'num_dependents', 'own_telephone', 'foreign_worker',\n",
      "       'target'],\n",
      "      dtype='object')\n",
      "Encoded DataFrame shape: (600, 26)\n",
      "Encoded DataFrame columns: Index(['duration', 'credit_amount', 'installment_commitment',\n",
      "       'residence_since', 'age', 'existing_credits', 'num_dependents',\n",
      "       'target', 'checking_status_<0', 'checking_status_0<=X<200',\n",
      "       'checking_status_>=200', 'checking_status_no checking',\n",
      "       'savings_status_<100', 'savings_status_100<=X<500',\n",
      "       'savings_status_500<=X<1000', 'savings_status_>=1000',\n",
      "       'savings_status_no known savings', 'employment_unemployed',\n",
      "       'employment_<1', 'employment_1<=X<4', 'employment_4<=X<7',\n",
      "       'employment_>=7', 'own_telephone_none', 'own_telephone_yes',\n",
      "       'foreign_worker_yes', 'foreign_worker_no'],\n",
      "      dtype='object')\n",
      "Encoded Train set shape: (300, 26)\n",
      "Encoded Test set shape: (250, 26)\n",
      "Encoded Validation set shape: (50, 26)\n",
      "Train set shape: (300, 13)\n",
      "Test set shape: (250, 13)\n",
      "Validation set shape: (50, 13)\n",
      "\n",
      "Target distribution in train set:\n",
      "0.0    0.516667\n",
      "1.0    0.483333\n",
      "Name: target, dtype: float64\n",
      "\n",
      "Target distribution in test set:\n",
      "1.0    0.5\n",
      "0.0    0.5\n",
      "Name: target, dtype: float64\n",
      "\n",
      "Target distribution in validation set:\n",
      "1.0    0.6\n",
      "0.0    0.4\n",
      "Name: target, dtype: float64\n",
      "Train set shape: (300, 13)\n",
      "Test set shape: (250, 13)\n",
      "Validation set shape: (50, 13)\n",
      "\n",
      "Target distribution in train set:\n",
      "0.0    0.516667\n",
      "1.0    0.483333\n",
      "Name: target, dtype: float64\n",
      "\n",
      "Target distribution in test set:\n",
      "1.0    0.5\n",
      "0.0    0.5\n",
      "Name: target, dtype: float64\n",
      "\n",
      "Target distribution in validation set:\n",
      "1.0    0.6\n",
      "0.0    0.4\n",
      "Name: target, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lowprofool/lib/python3.7/site-packages/sklearn/datasets/_openml.py:421: UserWarning: Multiple active versions of the dataset matching the name credit-g exist. Versions may be fundamentally different, returning version 1.\n",
      "  \" {version}.\".format(name=name, version=res[0][\"version\"])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1360/1983240253.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Train neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1360/3055884549.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(conf, load)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "# Load initial dataset\n",
    "df_orig, target, feature_names = get_df(DATASET)\n",
    "\n",
    "# Balance dataset classes\n",
    "df = balance_df(df_orig, target)\n",
    "\n",
    "# Compute the bounds for clipping\n",
    "bounds = get_bounds(df_orig, feature_names)\n",
    "\n",
    "# Normalize the data\n",
    "scaler, df, bounds = normalize(df, target, feature_names, bounds)\n",
    "\n",
    "# デバッグ\n",
    "print(\"Target value:\", target)\n",
    "print(\"DataFrame columns:\", df.columns)\n",
    "print(\"Is target in columns:\", target in df.columns)\n",
    "df['target'] = df['target'].astype(float)\n",
    "print(df['target'].dtype)\n",
    "print(df['target'].value_counts())\n",
    "# Compute the weihts modelizing the expert's knowledge\n",
    "try:\n",
    "    weights = get_weights(df, target)\n",
    "except Exception as e:\n",
    "    print(f\"Error in get_weights: {e}\")\n",
    "    print(f\"DataFrame columns: {df.columns}\")\n",
    "    print(f\"Target column: {target}\")\n",
    "    raise\n",
    "\n",
    "# デバッグ用\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(\"DataFrame columns:\", df.columns)\n",
    "\n",
    "# Split df into train/test/valid\n",
    "# df_train, df_test, df_valid = split_train_test_valid()\n",
    "\n",
    "categorical_features = ['checking_status', 'savings_status', 'employment', 'own_telephone', 'foreign_worker']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_features)\n",
    "\n",
    "print(\"Encoded DataFrame shape:\", df_encoded.shape)\n",
    "print(\"Encoded DataFrame columns:\", df_encoded.columns)\n",
    "\n",
    "# データの分割\n",
    "df_train_encoded, df_test_encoded, df_valid_encoded = split_train_test_valid(df_encoded)\n",
    "\n",
    "print(\"Encoded Train set shape:\", df_train_encoded.shape)\n",
    "print(\"Encoded Test set shape:\", df_test_encoded.shape)\n",
    "print(\"Encoded Validation set shape:\", df_valid_encoded.shape)\n",
    "\n",
    "# 分割後のデータセットのサイズを確認\n",
    "print(\"Train set shape:\", df_train.shape)\n",
    "print(\"Test set shape:\", df_test.shape)\n",
    "print(\"Validation set shape:\", df_valid.shape)\n",
    "\n",
    "# 各セットのターゲット変数の分布を確認\n",
    "print(\"\\nTarget distribution in train set:\")\n",
    "print(df_train['target'].value_counts(normalize=True))\n",
    "print(\"\\nTarget distribution in test set:\")\n",
    "print(df_test['target'].value_counts(normalize=True))\n",
    "print(\"\\nTarget distribution in validation set:\")\n",
    "print(df_valid['target'].value_counts(normalize=True))# データの分割\n",
    "df_train, df_test, df_valid = split_train_test_valid(df)\n",
    "\n",
    "# 分割後のデータセットのサイズを確認\n",
    "print(\"Train set shape:\", df_train.shape)\n",
    "print(\"Test set shape:\", df_test.shape)\n",
    "print(\"Validation set shape:\", df_valid.shape)\n",
    "\n",
    "# 各セットのターゲット変数の分布を確認\n",
    "print(\"\\nTarget distribution in train set:\")\n",
    "print(df_train['target'].value_counts(normalize=True))\n",
    "print(\"\\nTarget distribution in test set:\")\n",
    "print(df_test['target'].value_counts(normalize=True))\n",
    "print(\"\\nTarget distribution in validation set:\")\n",
    "print(df_valid['target'].value_counts(normalize=True))\n",
    "\n",
    "# Build experimenation config\n",
    "config = {'Dataset'     : 'credit-g',\n",
    "         'MaxIters'     : 20000,\n",
    "         'Alpha'        : 0.001,\n",
    "         'Lambda'       : 8.5,\n",
    "         'TrainData'    : df_train,\n",
    "         'TestData'     : df_test,\n",
    "         'ValidData'    : df_valid,\n",
    "         'Scaler'       : scaler,\n",
    "         'FeatureNames' : feature_names,\n",
    "         'Target'       : target,\n",
    "         'Weights'      : weights,\n",
    "         'Bounds'       : bounds}\n",
    "\n",
    "# Train neural network\n",
    "model = get_model(config)\n",
    "config['Model'] = model\n",
    "\n",
    "# Compute accuracy on test set\n",
    "y_true = df_test[target]\n",
    "x_test = torch.FloatTensor(df_test[feature_names].values)\n",
    "y_pred = model(x_test)\n",
    "y_pred = np.argmax(y_pred.detach().numpy(), axis=1)\n",
    "print(\"Accuracy score on test data\", accuracy_score(y_true, y_pred))\n",
    "    \n",
    "# Sub sample\n",
    "config['TestData'] = config['TestData'].sample(n=10, random_state = SEED)\n",
    "\n",
    "# Generate adversarial examples\n",
    "df_adv_lpf = gen_adv(config, 'LowProFool')\n",
    "df_adv_df = gen_adv(config, 'Deepfool')\n",
    "config['AdvData'] = {'LowProFool' : df_adv_lpf, 'Deepfool' : df_adv_df}\n",
    "\n",
    "# Compute metrics\n",
    "list_metrics = {'SuccessRate' : True,\n",
    "                'iter_means': False,\n",
    "                'iter_std': False,\n",
    "                'normdelta_median': False,\n",
    "                'normdelta_mean': True,\n",
    "                'n_std': True,\n",
    "                'weighted_median': False,\n",
    "                'weighted_mean': True,\n",
    "                'w_std': True,\n",
    "                'mean_dists_at_org': False,\n",
    "                'median_dists_at_org': False,\n",
    "                'mean_dists_at_tgt': False,\n",
    "                'mean_dists_at_org_weighted': True,\n",
    "                'mdow_std': True,\n",
    "                'median_dists_at_org_weighted': False,\n",
    "                'mean_dists_at_tgt_weighted': True,\n",
    "                'mdtw_std': True,\n",
    "                'prop_same_class_arg_org': False,\n",
    "                'prop_same_class_arg_adv': False}\n",
    "\n",
    "all_metrics = get_metrics(config, list_metrics)\n",
    "all_metrics = pd.DataFrame(all_metrics, columns=['Method'] + [k for k, v in list_metrics.items() if v])\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_341/4044212574.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplot_ratios\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mm_lpf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LowProFool'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mm_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethod\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'Deepfool'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "plot_ratios = []\n",
    "\n",
    "m_lpf = all_metrics[all_metrics.Method == 'LowProFool']\n",
    "m_df = all_metrics[all_metrics.Method =='Deepfool']\n",
    "\n",
    "sr = m_lpf.SuccessRate.values / m_df.SuccessRate.values \n",
    "wm =  m_lpf.weighted_mean.values / m_df.weighted_mean.values \n",
    "\n",
    "plot_ratios.append([100*sr[0], 100*wm[0]])\n",
    "plot_ratios = pd.DataFrame(plot_ratios, columns=['Success Rate Ratio', 'Mean Perturbation Ratio'])\n",
    "plot_ratios['Dataset'] = 'German Credit'\n",
    "\n",
    "f = plt.figure()\n",
    "ax = plt.axes()\n",
    "plot_ratios.plot(x='Dataset', kind='bar', legend=True, ax=ax)\n",
    "\n",
    "for i, v in enumerate(plot_ratios['Success Rate Ratio'].values):\n",
    "    ax.text(i - 0.2, v - 12 , str(v.round(1)) + '%', fontsize=16, color='white', weight='bold')\n",
    "for i, v in enumerate(plot_ratios['Mean Perturbation Ratio'].values):\n",
    "    ax.text(i + 0.062, v - 12, str(v.round(1)) + '%', fontsize=16, color='white', weight='bold')\n",
    "\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=0, ha='center')\n",
    "ax.axhline(100, ls=':', c='grey')\n",
    "ax.text(-0.49, 100 - 5, '100%')\n",
    "\n",
    "\n",
    "ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
